# Local AI Study Assistant

A terminal-based AI study assistant built using Python and Ollama that runs a locally hosted LLaMA language model.  
The application allows users to ask study-related questions directly from the command line and receive clear, contextual explanations â€” completely offline and without relying on cloud APIs.

This project demonstrates how to integrate local large language models into Python applications, design interactive CLI tools, and build AI-powered learning utilities using terminal-based workflows.

## Why This Project?

Most beginners interact with AI only through web interfaces.  
This project focuses on running and controlling a large language model locally, giving full control over data, performance, and interaction flow.

It highlights practical skills such as:
- Local LLM deployment
- Python-based AI integration
- Command-line application design
- Prompt handling and response parsing

## Technologies Used
- Python
- Ollama
- LLaMA (local model)
- Terminal / CLI
- Git & GitHub

## Intended Use

- Students learning Python or machine learning concepts
- Developers exploring local AI workflows
- Anyone interested in offline AI-powered study tools

